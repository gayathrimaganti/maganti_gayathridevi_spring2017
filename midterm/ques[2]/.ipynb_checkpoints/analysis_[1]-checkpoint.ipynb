{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Question 2 ANALYSIS 1 : (55 points)\n",
    "\n",
    "\n",
    "> Use NYT API to collect NYT data. Perform 3 analysis on the collected data.\n",
    "\n",
    "Link to NYT developer docs : [NYT API Documentation](http://developer.nytimes.com/)\n",
    "\n",
    "\n",
    "| Code                 | Points           | \n",
    "| -------------        |:-------------:   | \n",
    "| Collect Data         | **10** Points    | \n",
    "| Storing Data         | **15** Points    |   \n",
    "| Individual Analysis  | **10** Points    |  \n",
    "\n",
    "#### Instructions :\n",
    "- You would need to create an API key.\n",
    "- Use `request` or `beautiful-soap` library to download the data. (No other library or crawler allowed).\n",
    "- Store the data in your local machine.\n",
    "- Your analysis should use **this downloaded data only** (and not try to redownload this data again during analysis time).\n",
    "- There is a rate limit for downloading the data. I would suggest you to start collecting the data from day 1. You can try  \n",
    "  using multiple account to get more than 1 key.\n",
    "- You need to use atleast 2 API method eg: `archive`, `Article Search`. **Do not use** `Movies Review`, `Semantic` API.\n",
    "\n",
    "# General Instructions :\n",
    "\n",
    "- You need to submit a `runnable ipython notebook`. TA should be able to clone your repository and run the code in their \n",
    "  machine. (They will install any libraries you have as well as set-up any environment variable and file argument that you have  \n",
    "  need.) But there should be no code change required to run the notebook.\n",
    "- You are allowed to use any python libraries except **Numpy,Pandas,automated crawler tools**. You can use a library for  \n",
    "  crawling (if you think you need it but it should not be an automated click to run types.)\n",
    "- Do not share/upload any keys on Github. You should store them as environment variable.\n",
    "```sh\n",
    "$ export nyt_archive_key = abcd1234\n",
    "```\n",
    "```python\n",
    "import os\n",
    "nyt_archive_key = os.get_env('nyt_archive_key')\n",
    "```\n",
    "- You can only use excel, matplotlib, seaborn to create plots (**optional to create plots**).\n",
    "- Following folder/file structure is preferred\n",
    "\n",
    "\n",
    "| Path                                | Purpose                                                                         | \n",
    "| ------------------------------------|---------------------------------------------------------------------------------| \n",
    "|`midterm/`                           | folder to store all your midterm submission files                               | \n",
    "| `midterm/data/*`                    | Store all raw data                                                              |   \n",
    "| `midterm/que[1-2]/ana_[1-3].ipynb`  | Notebook to store the code for analysis                                         |\n",
    "| `midterm/que[1-2]/ana_[1-3]/`       | Extra files required/generated for each analysis (Eg. output.csv,plot.png)      |\n",
    "| `midterm/readme.md`                 | Markdown report                                                                 |\n",
    "\n",
    "-  The TA will only look for a folder `midterm` in your repository and all the required files should be available inside it. \n",
    "   You may lose points if the files are kept in some other location.\n",
    "-  Using **NLTK** in your analysis will carry higher points. Just using it for the sake of it does not constitute as NLTK usage.\n",
    "-  Since this is mid-term, **no extension of deadline is available**. You lose 5 point for every 4 hours delay.\n",
    "   Eg : `You submit it at 12:01 AM or 3:01 AM , you will lose 5 points. \n",
    "   You submit it at 4:01 AM you lose 10 points`\n",
    "-  Please use **Markdown syntax in your ipynb notebook** so the TA understands what you are trying to do. \n",
    "-  You will need to create a final report stating what analysis you have done and its output. It needs to be created as a        \n",
    "   markdown document as named `readme.md`. This is important and failing to do so will result in large loss of marks. \n",
    "   **`No Prezi,ppt,pdf allowed`**. \n",
    "   Imagine if I was to go through only this document (and no other file), I should be able to understand what data you had, how    you obtained it, how is it stored, what analysis have you done, what information did you get etc etc.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "nyt_apikey='52499dd2eba145d199083b6f96c66918'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import requests                                             \n",
    "import json                                         \n",
    "import os                                                   \n",
    "import time                                             \n",
    "\n",
    "for i in range (0,121):                             \n",
    "    apikey_param={'api-key':os.environ['auth_key'],'page':i}\n",
    "                                         \n",
    "    res_articlesearch=req.get('https://api.nytimes.com/svc/search/v2/articlesearch.json',params=apikey_param)\n",
    "    print(res_articlesearch)                       \n",
    "    print(res_articlesearch.url)                   \n",
    "    \n",
    "    with open(\"data/ArticleSearch/response_articlesearch_data\"+str(i)+\".json\",\"a\") as Resp_data:\n",
    "        json.dump(res_articlesearch.json(),Resp_data)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Collecting Data from Archive API"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "import requests                                         \n",
    "import json                                                 \n",
    "import os                                                   \n",
    "import time                                             \n",
    "\n",
    "for i in range (0,120):                                 \n",
    "    apikey_param={'api-key':os.environ['auth_key'],'page':i}\n",
    "                                                        \n",
    "    time.sleep(4)                                           \n",
    "    \n",
    "    res_archive=req.get('https://api.nytimes.com/svc/archive/v1/2016/1.json',params=apikey_param)\n",
    "                                                        \n",
    "    print(res_archive)              \n",
    "    print(res_archive.url)                  \n",
    "    with open(\"res_archive4.json\",\"a\") as Resp_data:\n",
    "                                                            \n",
    "        json.dump(res_archive.json(),Resp_data)  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of articles for each section\n",
      "{}\n"
     ]
    }
   ],
   "source": [
    "from glob import glob                               \n",
    "list_sec = [];                                          \n",
    "import json\n",
    "\n",
    "d={}                                                         \n",
    "for fname in glob('data/ArticleSearch/*.json'):          \n",
    "    with open(fname) as r:                               \n",
    "        json_data = json.load(r)                            \n",
    "        json_resp=json_data['response']                  \n",
    "        json_docs=json_resp['docs']                      \n",
    "        for doc in json_docs:\n",
    "            if doc['sec_name'] not in list_sec:  \n",
    "                list_sec.append(doc['sec_name'])\n",
    "for i in range(0,len(list_sec)):\n",
    "    count=0\n",
    "    for fname in glob('data/article_search/*.json'):     \n",
    "        with open(fname) as r:                            \n",
    "            json_data = json.load(r)                         \n",
    "            json_resp=json_data['response']\n",
    "            json_docs=json_resp['docs']\n",
    "            for doc in json_docs:\n",
    "                if doc['sec_name']==list_sec[i]:    \n",
    "                    count+=1\n",
    "                    d[doc['sec_name']]=count\n",
    "print(\"Number of articles for each section\")\n",
    "print(d)     "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "for each_sec in list_sec:\n",
    "    list_subsec=[]\n",
    "    for fname in glob('data/ArticleSearch/*.json'):          \n",
    "        with open(fname) as r:                              \n",
    "            json_data = json.load(r)                             \n",
    "            json_resp=json_data['response']\n",
    "            json_docs=json_resp['docs']\n",
    "            for doc in json_docs:\n",
    "                    if doc['sec_name']==each_sec:\n",
    "                        if doc['subsec_name'] not in list_subsec:\n",
    "                            list_subsec.append(doc['subsec_name'])\n",
    "    print(each_sec)\n",
    "    print(list_subsec)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "list_subsecs=['Middle East', 'Asia Pacific', 'Europe', 'Africa', 'Americas', None, 'Canada']\n",
    "dsubsec={}\n",
    "for i_in in range(len(list_subsecs)):\n",
    "    count=0\n",
    "    for fname in glob('data/ArticleSearch/*.json'):    \n",
    "        with open(fname) as r:                            \n",
    "            json_data = json.load(r)                \n",
    "            json_res=json_data['response']\n",
    "            json_docs=json_res['docs']\n",
    "            for doc in json_docs:\n",
    "                if doc['sec_name']==\"World\":         \n",
    "                    if doc['subsec_name']==list_subsecs[i_in]:\n",
    "                        count+=1\n",
    "                        dsubsec[doc['subsec_name']]=count \n",
    "#print(\"World\")\n",
    "regions=[]\n",
    "aricles_region=[]\n",
    "for d_sub in dsubsec.items():\n",
    "#    print(d_sub)\n",
    "    regions.append(d_sub[0])\n",
    "    articles_region.append(d_sub[1])\n",
    "\n",
    "#print(regions)\n",
    "#print(articles_region)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from collections import Counter\n",
    "import nltk\n",
    "from nltk.corpus import stopwords\n",
    "import string\n",
    "\n",
    "counter=0\n",
    "headlines=[]\n",
    "for fname in glob('data/ArticleSearch/*.json'):      \n",
    "    with open(fname) as r:                            \n",
    "        json_data = json.load(r)                         \n",
    "        json_resp=json_data['response']\n",
    "        json_docs=json_resp['docs']\n",
    "        for doc in json_docs:\n",
    "            if doc['sec_name']==\"World\":             \n",
    "                if doc['subsec_name']==\"Europe\":\n",
    "                    headl=doc['headline']\n",
    "                    print_headline=headl['main']\n",
    "                    headlines.append(print_headline)\n",
    "                    counter+=1;\n",
    "                \n",
    "\n",
    "ignoreChar=['\\r','\\n','',' ',\"'s\"]\n",
    "nums=['0','1','2','3','4','5','6','7','8','9']\n",
    "hl_data = nltk.word_tokenize(' '.join(headlines) )\n",
    "words_only = [l.lower() for l in hl_data if l not in string.punctuation if l not in ignoreChar if l not in nums]\n",
    "filtered_hl_data=[word for word in words_only if word not in stopwords.words('english')]\n",
    "wnl = nltk.WordNetLemmatizer()\n",
    "counts=Counter([wnl.lemmatize(data) for data in filtered_hl_data])\n",
    "commn_words=[]\n",
    "freq=[]\n",
    "for a in counts.most_common(30):\n",
    "    commn_words.append(a[0])\n",
    "    freq.append(a[1])\n",
    "    print(a)"
   ]
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "kernelspec": {
   "display_name": "Python [conda root]",
   "language": "python",
   "name": "conda-root-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
