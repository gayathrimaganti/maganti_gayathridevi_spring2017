{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Question 2 ANALYSIS 3 : (55 points)\n",
    "\n",
    "\n",
    "> Use NYT API to collect NYT data. Perform 3 analysis on the collected data.\n",
    "\n",
    "Link to NYT developer docs : [NYT API Documentation](http://developer.nytimes.com/)\n",
    "\n",
    "\n",
    "| Code                 | Points           | \n",
    "| -------------        |:-------------:   | \n",
    "| Collect Data         | **10** Points    | \n",
    "| Storing Data         | **15** Points    |   \n",
    "| Individual Analysis  | **10** Points    |  \n",
    "\n",
    "#### Instructions :\n",
    "- You would need to create an API key.\n",
    "- Use `request` or `beautiful-soap` library to download the data. (No other library or crawler allowed).\n",
    "- Store the data in your local machine.\n",
    "- Your analysis should use **this downloaded data only** (and not try to redownload this data again during analysis time).\n",
    "- There is a rate limit for downloading the data. I would suggest you to start collecting the data from day 1. You can try  \n",
    "  using multiple account to get more than 1 key.\n",
    "- You need to use atleast 2 API method eg: `archive`, `Article Search`. **Do not use** `Movies Review`, `Semantic` API.\n",
    "\n",
    "# General Instructions :\n",
    "\n",
    "- You need to submit a `runnable ipython notebook`. TA should be able to clone your repository and run the code in their \n",
    "  machine. (They will install any libraries you have as well as set-up any environment variable and file argument that you have  \n",
    "  need.) But there should be no code change required to run the notebook.\n",
    "- You are allowed to use any python libraries except **Numpy,Pandas,automated crawler tools**. You can use a library for  \n",
    "  crawling (if you think you need it but it should not be an automated click to run types.)\n",
    "- Do not share/upload any keys on Github. You should store them as environment variable.\n",
    "```sh\n",
    "$ export nyt_archive_key = abcd1234\n",
    "```\n",
    "```python\n",
    "import os\n",
    "nyt_archive_key = os.get_env('nyt_archive_key')\n",
    "```\n",
    "- You can only use excel, matplotlib, seaborn to create plots (**optional to create plots**).\n",
    "- Following folder/file structure is preferred\n",
    "\n",
    "\n",
    "| Path                                | Purpose                                                                         | \n",
    "| ------------------------------------|---------------------------------------------------------------------------------| \n",
    "|`midterm/`                           | folder to store all your midterm submission files                               | \n",
    "| `midterm/data/*`                    | Store all raw data                                                              |   \n",
    "| `midterm/que[1-2]/ana_[1-3].ipynb`  | Notebook to store the code for analysis                                         |\n",
    "| `midterm/que[1-2]/ana_[1-3]/`       | Extra files required/generated for each analysis (Eg. output.csv,plot.png)      |\n",
    "| `midterm/readme.md`                 | Markdown report                                                                 |\n",
    "\n",
    "-  The TA will only look for a folder `midterm` in your repository and all the required files should be available inside it. \n",
    "   You may lose points if the files are kept in some other location.\n",
    "-  Using **NLTK** in your analysis will carry higher points. Just using it for the sake of it does not constitute as NLTK usage.\n",
    "-  Since this is mid-term, **no extension of deadline is available**. You lose 5 point for every 4 hours delay.\n",
    "   Eg : `You submit it at 12:01 AM or 3:01 AM , you will lose 5 points. \n",
    "   You submit it at 4:01 AM you lose 10 points`\n",
    "-  Please use **Markdown syntax in your ipynb notebook** so the TA understands what you are trying to do. \n",
    "-  You will need to create a final report stating what analysis you have done and its output. It needs to be created as a        \n",
    "   markdown document as named `readme.md`. This is important and failing to do so will result in large loss of marks. \n",
    "   **`No Prezi,ppt,pdf allowed`**. \n",
    "   Imagine if I was to go through only this document (and no other file), I should be able to understand what data you had, how    you obtained it, how is it stored, what analysis have you done, what information did you get etc etc.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import requests as req\n",
    "import urllib.parse\n",
    "import json\n",
    "api_key='52499dd2eba145d199083b6f96c66918'\n",
    "date = 20111201\n",
    "page = 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2\n"
     ]
    }
   ],
   "source": [
    "url = \"https://api.nytimes.com/svc/search/v2/articlesearch.json?begin_date=\" + str(date) + \"&end_date=\" + str(date) + \"&page=\" + str(page) + \"&api-key=\" + str(api_key)\n",
    "res =  req.get(url).json()\n",
    "with open(str(date) + '_article_' + str(page)+ '.json', 'w') as fp:\n",
    "    json.dump(res, fp)\n",
    "    page += 1\n",
    "    date += 1\n",
    "    print(page)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import os\n",
    "newpath = r'\\Users\\gayat\\midterm\\data\\ArticleSearch' \n",
    "if not os.path.exists(newpath):\n",
    "    os.makedirs(newpath)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from shutil import copy\n",
    "from collections import Counter\n",
    "rootdir = '\\Users\\gayat\\midterm\\data\\ArticleSearch'\n",
    "for root, dirs, files in os.walk(rootdir):\n",
    "    for name in files:\n",
    "         if name.endswith((\".json\")):\n",
    "            #print(name)\n",
    "            copy(rootdir +'\\\\'+name, newpath)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import os, json\n",
    "import pandas as pd\n",
    "import shutil\n",
    "path_to_json = '/Users/gayat/midterm/data/ArticleSearch'\n",
    "path_file= '\\\\Users\\\\gayat\\\\midterm\\\\data\\\\ArticleSearch\\\\Result'\n",
    "\n",
    "json_files = [pos_json for pos_json in os.listdir(path_to_json) if pos_json.endswith('.json')]\n",
    "#print (json_files) \n",
    "\n",
    "def ensure_dir(path_file):\n",
    "    #direc = os.path.dirname(path_file)\n",
    "    #print(path_file)\n",
    "    if not os.path.exists(path_file):\n",
    "        os.makedirs(path_file)\n",
    "        #print(path_file)\n",
    "    #return direc"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import json\n",
    "import glob\n",
    "file = \"*.json\"\n",
    "\n",
    "from shutil import copy\n",
    "for js in glob.glob(os.path.join(path_to_json,file)):\n",
    "            json_file_read = json.loads(open(js).read())\n",
    "            for key,value in json_file_read.items():\n",
    "                #print(key)\n",
    "                if key == \"response\": # creation of folders\n",
    "                    \n",
    "                    meta = value.get('meta')\n",
    "                    #print(meta)\n",
    "                    docs = value.get('docs')\n",
    "                    for article in docs :\n",
    "                        sect_name = article.get('section_name')\n",
    "                        sect_name = str(sect_name).replace(\"/\",\" \")\n",
    "                        sect_name = str(sect_name).replace(\":\",\" \")\n",
    "                        #hits = meta.get('hits')\n",
    "                        path = os.path.join(path_file + '\\\\' + str(sect_name))\n",
    "                        #path = os.path.join(path + '/' + hits)\n",
    "                        #print(path)\n",
    "                        ensure_dir(path)\n",
    "                        copy(js,path)"
   ]
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "kernelspec": {
   "display_name": "Python [conda root]",
   "language": "python",
   "name": "conda-root-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
